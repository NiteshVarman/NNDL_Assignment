{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRS3vEvAWCML",
        "outputId": "78db6cb9-c8de-4185-fdf4-68e556516c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Implementation\n",
            "\n",
            "OR Gate:\n",
            "0 OR 0 = 0\n",
            "0 OR 1 = 1\n",
            "1 OR 0 = 1\n",
            "1 OR 1 = 1\n",
            "\n",
            "AND Gate:\n",
            "0 AND 0 = 0\n",
            "0 AND 1 = 0\n",
            "1 AND 0 = 0\n",
            "1 AND 1 = 1\n",
            "\n",
            "XOR Gate:\n",
            "0 XOR 0 = 0\n",
            "0 XOR 1 = 1\n",
            "1 XOR 0 = 1\n",
            "1 XOR 1 = 0\n"
          ]
        }
      ],
      "source": [
        "print(\"Perceptron Implementation\")\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Basic single-layer perceptrons for logic gates\n",
        "def perceptron(x1, x2, w1, w2, bias):\n",
        "    return step(w1 * x1 + w2 * x2 + bias)\n",
        "\n",
        "# OR Gate using perceptron\n",
        "print(\"\\nOR Gate:\")\n",
        "for a in [0, 1]:\n",
        "    for b in [0, 1]:\n",
        "        print(f\"{a} OR {b} = {perceptron(a, b, 2, 2, -1.0)}\")\n",
        "\n",
        "# AND Gate using perceptron\n",
        "print(\"\\nAND Gate:\")\n",
        "for a in [0, 1]:\n",
        "    for b in [0, 1]:\n",
        "        print(f\"{a} AND {b} = {perceptron(a, b, 2, 2, -3.0)}\")\n",
        "\n",
        "# XOR Gate using multi-layer logic (OR + NAND → AND)\n",
        "def xor_gate(x1, x2):\n",
        "    h1 = perceptron(x1, x2, 1, 1, -0.5)    # OR\n",
        "    h2 = perceptron(x1, x2, -2, -2, 3)     # NAND\n",
        "    return perceptron(h1, h2, 1, 1, -1.5)  # AND of h1 and h2\n",
        "\n",
        "print(\"\\nXOR Gate:\")\n",
        "for a in [0, 1]:\n",
        "    for b in [0, 1]:\n",
        "        print(f\"{a} XOR {b} = {xor_gate(a, b)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PyTorch MLP Implementation\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Simple MLP: 2 inputs → 4 hidden units → 1 output\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 4), nn.ReLU(),\n",
        "            nn.Linear(4, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Function to train and test gate models\n",
        "def train_gate(name, x, y, epochs=1000, lr=0.5):\n",
        "    model = MLP()\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = (model(x) > 0.5).float()\n",
        "        print(f\"\\n{name} GATE RESULTS\\nInput\\tPred\\tActual\")\n",
        "        for i in range(4):\n",
        "            print(f\"{x[i].tolist()}\\t{int(pred[i])}\\t{int(y[i])}\")\n",
        "\n",
        "# Input and truth tables\n",
        "inputs = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)\n",
        "labels = {\n",
        "    \"AND\": torch.tensor([[0],[0],[0],[1.]], dtype=torch.float32),\n",
        "    \"OR\" : torch.tensor([[0],[1],[1],[1.]], dtype=torch.float32),\n",
        "    \"NOR\": torch.tensor([[1],[0],[0],[0.]], dtype=torch.float32),\n",
        "}\n",
        "\n",
        "# Train all gates\n",
        "for gate in labels:\n",
        "    train_gate(gate, inputs, labels[gate])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uyVCMVNWLxa",
        "outputId": "ed54cdf5-8067-4e4d-cde9-f27119aa0347"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch MLP Implementation\n",
            "\n",
            "AND GATE RESULTS\n",
            "Input\tPred\tActual\n",
            "[0.0, 0.0]\t0\t0\n",
            "[0.0, 1.0]\t0\t0\n",
            "[1.0, 0.0]\t0\t0\n",
            "[1.0, 1.0]\t1\t1\n",
            "\n",
            "OR GATE RESULTS\n",
            "Input\tPred\tActual\n",
            "[0.0, 0.0]\t0\t0\n",
            "[0.0, 1.0]\t1\t1\n",
            "[1.0, 0.0]\t1\t1\n",
            "[1.0, 1.0]\t1\t1\n",
            "\n",
            "NOR GATE RESULTS\n",
            "Input\tPred\tActual\n",
            "[0.0, 0.0]\t1\t1\n",
            "[0.0, 1.0]\t0\t0\n",
            "[1.0, 0.0]\t0\t0\n",
            "[1.0, 1.0]\t0\t0\n"
          ]
        }
      ]
    }
  ]
}